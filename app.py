import streamlit as st
import google.generativeai as genai
from google.api_core import exceptions
from PIL import Image
import time
import os

# ---------------------------------------------------------
# è¨­å®šãƒ»é–¢æ•°å®šç¾©
# ---------------------------------------------------------
st.set_page_config(page_title="TTV Quality Gatekeeper", page_icon="ğŸ›¡ï¸", layout="wide")

def load_knowledge_base():
    try:
        with open("knowledge_base.txt", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "ã‚¨ãƒ©ãƒ¼ï¼šknowledge_base.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

# ---------------------------------------------------------
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šèªè¨¼ & è¨­å®š
# ---------------------------------------------------------
with st.sidebar:
    st.header("èªè¨¼è¨­å®š")
    user_password = st.text_input("ã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼", type="password")
    if user_password != st.secrets["APP_PASSWORD"]:
        st.warning("âš ï¸ æ­£ã—ã„ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()
    
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    st.success("èªè¨¼æˆåŠŸ")
    
    st.divider()
    with st.expander("ç¾åœ¨ã®ãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ«"):
        st.text(load_knowledge_base())
    
    if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ¶ˆå»"):
        st.session_state.messages = []
        st.rerun()

# ---------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³ç”»é¢
# ---------------------------------------------------------
st.title("ğŸ›¡ï¸ TTV Quality Gatekeeper")
st.info("""
TTVã®æœ€æ–°å±æ©Ÿç®¡ç†è¦å®šã«åŸºãã€å‹•ç”»ãƒ»ç”»åƒã®ãƒã‚§ãƒƒã‚¯ãŠã‚ˆã³è¦å®šã«é–¢ã™ã‚‹ç…§ä¼šã‚’è¡Œã„ã¾ã™ã€‚

â€»æœ¬ãƒ„ãƒ¼ãƒ«ã¯éå»ã®äº‹ä¾‹ã‚„ãƒŠãƒ¬ãƒƒã‚¸ã«åŸºã¥ãã€ãƒªã‚¹ã‚¯è¦å› ã‚’æŠ½å‡ºãƒ»æç¤ºã™ã‚‹æ”¯æ´ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
æœ€çµ‚çš„ãªå…¬é–‹å¯å¦ã®åˆ¤æ–­ã¯å¿…ãšäººé–“ã®ç›®è¦–ã«ã‚ˆã£ã¦è¡Œã£ã¦ãã ã•ã„ã€‚
""")

tab1, tab2 = st.tabs(["ğŸ“ ç´ æãƒã‚§ãƒƒã‚¯ (å‹•ç”»/ç”»åƒ)", "ğŸ’¬ è¦å®šç…§ä¼šãƒãƒ£ãƒƒãƒˆ"])

# =========================================================
# ã‚¿ãƒ–1ï¼šç´ æãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ (å‹•ç”» & ç”»åƒ)
# =========================================================
with tab1:
    st.subheader("ãƒ¡ãƒ‡ã‚£ã‚¢å“è³ªãƒã‚§ãƒƒã‚¯")
    uploaded_file = st.file_uploader(
        "ãƒã‚§ãƒƒã‚¯ã—ãŸã„ãƒ•ã‚¡ã‚¤ãƒ« (MP4, MOV, JPG, PNG) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=["mp4", "mov", "jpg", "jpeg", "png", "webp"]
    )

    if uploaded_file is not None:
        file_type = uploaded_file.type
        current_knowledge = load_knowledge_base()
        
        # --- ç”»åƒã®å ´åˆ ---
        if "image" in file_type:
            image = Image.open(uploaded_file)
            st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)
            
            if st.button("ğŸš€ ç”»åƒãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("ç”»åƒå†…ã®æ–‡å­—ã¨æå†™ã‚’è§£æä¸­..."):
                    try:
                        model = genai.GenerativeModel(model_name="gemini-flash-latest")
                        
                        # â˜…ã“ã“ãŒé‡è¦ï¼šè§£é‡ˆã‚’è¨±å¯ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                        prompt = f"""
                        ã‚ãªãŸã¯TTVã®å³æ ¼ãªæ ¡é–²ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ‹…å½“AIã§ã™ã€‚
                        ä»¥ä¸‹ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯ï¼‰ã«åŸºã¥ãã€ç”»åƒå†…ã®ã€Œæ–‡å­—ã€ã¨ã€Œæå†™ã€ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

                        â– åˆ¤å®šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆé‡è¦ï¼‰
                        1. **ãƒ«ãƒ¼ãƒ«ã®é©ç”¨:** ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«è¨˜è¼‰ã•ã‚ŒãŸç¦æ­¢äº‹é …ï¼ˆä¾‹ï¼šã€Œå·®åˆ¥è¡¨ç¾ã€ï¼‰ã«ã¤ã„ã¦ã¯ã€å…·ä½“çš„ãªè¨˜è¿°ãŒãªãã¦ã‚‚ã€ä¸€èˆ¬çš„å®šç¾©ã«ç…§ã‚‰ã—ã¦é•åï¼ˆä¾‹ï¼šã€Œè‚Œã®è‰²ã‚’æ¶æ„ã€ãªã©ï¼‰ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚
                        2. **ç¯„å›²ã®é™å®š:** ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«å…¨ãã‚«ãƒ†ã‚´ãƒªãŒå­˜åœ¨ã—ãªã„äº‹é …ï¼ˆä¾‹ï¼šãƒ«ãƒ¼ãƒ«ã«ãªã„ã€Œæœè£…ã®ã‚»ãƒ³ã‚¹ã€ã‚„ã€Œå€‹äººçš„ãªæ„Ÿæƒ³ã€ï¼‰ã«ã¤ã„ã¦ã¯ã€ä¸€åˆ‡æŒ‡æ‘˜ã—ãªã„ã§ãã ã•ã„ã€‚

                        â– ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹
                        {current_knowledge}

                        â– å‡ºåŠ›
                        é•åç®‡æ‰€ã®ã¿ã‚’ç®‡æ¡æ›¸ãã§æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚
                        é•åãŒãªã„å ´åˆã¯å¿…ãšã€ŒæŒ‡æ‘˜äº‹é …ãªã—ã€ã¨ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                        """
                        
                        response = model.generate_content([image, prompt])
                        st.success("è§£æå®Œäº†")
                        st.markdown("### ğŸ“Š ç”»åƒåˆ¤å®šãƒ¬ãƒãƒ¼ãƒˆ")
                        st.markdown(response.text)
                        
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # --- å‹•ç”»ã®å ´åˆ ---
        elif "video" in file_type:
            st.video(uploaded_file)
            
            if st.button("ğŸš€ å‹•ç”»ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ", type="primary"):
                status_text = st.empty()
                progress_bar = st.progress(0)

                try:
                    status_text.text("AIã‚µãƒ¼ãƒãƒ¼ã¸è»¢é€ä¸­...")
                    progress_bar.progress(20)
                    temp_file_path = "temp_video.mp4"
                    with open(temp_file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    video_file = genai.upload_file(path=temp_file_path)

                    while video_file.state.name == "PROCESSING":
                        status_text.text("æ˜ åƒå‡¦ç†ä¸­... (æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")
                        time.sleep(2)
                        video_file = genai.get_file(video_file.name)

                    if video_file.state.name == "FAILED":
                        st.error("å‹•ç”»å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.stop()

                    status_text.text("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã¨ç…§åˆä¸­...")
                    progress_bar.progress(60)
                    
                    model = genai.GenerativeModel(model_name="gemini-flash-latest")
                    
                    # â˜…ã“ã“ãŒé‡è¦ï¼šè§£é‡ˆã‚’è¨±å¯ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå‹•ç”»ç‰ˆï¼‰
                    prompt = f"""
                    ã‚ãªãŸã¯TTVã®å³æ ¼ãªæ ¡é–²ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ‹…å½“AIã§ã™ã€‚
                    ä»¥ä¸‹ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯ï¼‰ã«åŸºã¥ãå‹•ç”»ã‚’è§£æã—ã¦ãã ã•ã„ã€‚

                    â– åˆ¤å®šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆé‡è¦ï¼‰
                    1. **ãƒ«ãƒ¼ãƒ«ã®é©ç”¨:** ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«è¨˜è¼‰ã•ã‚ŒãŸç¦æ­¢äº‹é …ï¼ˆä¾‹ï¼šã€Œå·®åˆ¥è¡¨ç¾ã€ï¼‰ã«ã¤ã„ã¦ã¯ã€å…·ä½“çš„ãªè¨˜è¿°ãŒãªãã¦ã‚‚ã€ä¸€èˆ¬çš„å®šç¾©ã«ç…§ã‚‰ã—ã¦é•åï¼ˆä¾‹ï¼šã€Œè‚Œã®è‰²ã‚’æ¶æ„ã€ãªã©ï¼‰ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚
                    2. **ç¯„å›²ã®é™å®š:** ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«å…¨ãã‚«ãƒ†ã‚´ãƒªãŒå­˜åœ¨ã—ãªã„äº‹é …ï¼ˆä¾‹ï¼šãƒ«ãƒ¼ãƒ«ã«ãªã„ã€Œæœè£…ã®ã‚»ãƒ³ã‚¹ã€ã‚„ã€Œå€‹äººçš„ãªæ„Ÿæƒ³ã€ï¼‰ã«ã¤ã„ã¦ã¯ã€ä¸€åˆ‡ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚

                    â– ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹
                    {current_knowledge}

                    â– å‡ºåŠ›å½¢å¼ (Markdownãƒ†ãƒ¼ãƒ–ãƒ«)
                    é•åãŒãªã„å ´åˆã¯ã€ŒæŒ‡æ‘˜äº‹é …ãªã—ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                    
                    | ã‚¿ã‚¤ãƒ ã‚³ãƒ¼ãƒ‰ | åˆ¤å®š(NG/æ³¨æ„) | æŒ‡æ‘˜å†…å®¹ | è©²å½“ãƒŠãƒ¬ãƒƒã‚¸ |
                    | :--- | :--- | :--- | :--- |
                    """

                    try:
                        response = model.generate_content([video_file, prompt])
                    except exceptions.ResourceExhausted:
                        status_text.warning("âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹é›†ä¸­ã€‚30ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                        time.sleep(30)
                        status_text.text("å†è©¦è¡Œä¸­...")
                        response = model.generate_content([video_file, prompt])
                    
                    progress_bar.progress(100)
                    status_text.text("å®Œäº†")
                    
                    st.divider()
                    st.markdown("### ğŸ“Š å‹•ç”»åˆ¤å®šãƒ¬ãƒãƒ¼ãƒˆ")
                    st.markdown(response.text)
                    
                    st.warning("""
                    **TTVã®æœ€æ–°å±æ©Ÿç®¡ç†è¦å®šã«åŸºãå‹•ç”»ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚**
                    
                    æœ¬ãƒ„ãƒ¼ãƒ«ã¯éå»ã®äº‹ä¾‹ã‚„ãƒŠãƒ¬ãƒƒã‚¸ã«åŸºã¥ãã€ãƒªã‚¹ã‚¯è¦å› ã‚’æŠ½å‡ºãƒ»æç¤ºã™ã‚‹æ”¯æ´ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
                    **æœ€çµ‚çš„ãªå…¬é–‹å¯å¦ã®åˆ¤æ–­ã¯å¿…ãšäººé–“ã®ç›®è¦–ã«ã‚ˆã£ã¦è¡Œã£ã¦ãã ã•ã„ã€‚**
                    """)

                    genai.delete_file(video_file.name)
                    os.remove(temp_file_path)

                except Exception as e:
                    st.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

# =========================================================
# ã‚¿ãƒ–2ï¼šè¦å®šç…§ä¼šãƒãƒ£ãƒƒãƒˆ
# =========================================================
with tab2:
    st.subheader("ğŸ’¬ è¦å®šç…§ä¼šãƒãƒ£ãƒƒãƒˆ")
    st.caption("ç¾åœ¨ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã€Œãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ«ï¼ˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼‰ã€ã®å†…å®¹ã«ã¤ã„ã¦ã®ã¿å›ç­”ã—ã¾ã™ã€‚")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è¦å®šã«ã¤ã„ã¦è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                current_knowledge = load_knowledge_base()
                model = genai.GenerativeModel(model_name="gemini-flash-latest")
                
                # ãƒãƒ£ãƒƒãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚ã€Œé©ç”¨ã€ã¨ã€Œé™å®šã€ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹
                system_instruction = f"""
                ã‚ãªãŸã¯TTVã®ã€Œè¦å®šç…§ä¼šå°‚ç”¨AIã€ã§ã™ã€‚
                ä»¥ä¸‹ã®ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã€‘ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
                
                â– å›ç­”ãƒ«ãƒ¼ãƒ«
                1. è³ªå•å†…å®¹ãŒãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®é …ç›®ã®ã€Œå…·ä½“ä¾‹ã€ã§ã‚ã‚‹å ´åˆã¯ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ ¹æ‹ ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šã€Œè‚Œã®è‰²ã®æ¶æ„ã¯ãƒ€ãƒ¡ï¼Ÿã€â†’ã€Œå·®åˆ¥ã®ç¦æ­¢è¦å®šã«åŸºã¥ãNGã§ã™ã€ï¼‰
                2. è³ªå•å†…å®¹ã«é–¢é€£ã™ã‚‹é …ç›®ãŒãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«å…¨ãç„¡ã„å ´åˆã¯ã€ã€Œè¦å®šã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
                
                â– ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹
                {current_knowledge}
                """
                
                chat = model.start_chat(history=[])
                full_prompt = f"{system_instruction}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}"
                
                response = model.generate_content(full_prompt)
                
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
